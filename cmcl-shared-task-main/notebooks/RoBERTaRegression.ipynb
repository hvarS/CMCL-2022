{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RoBERTa Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tqdm\n",
    "import torch\n",
    "from collections import defaultdict, Counter\n",
    "import random\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "import src.eval_metric\n",
    "import src.model\n",
    "import src.dataloader\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "pd.options.display.max_columns = 100\n",
    "pd.options.display.max_rows = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../../data/training_data/train.csv\")\n",
    "valid_df = pd.read_csv(\"../../data/training_data/dev.csv\")\n",
    "test_df = pd.read_csv(\"../../data/test_data_subtask2/sub2/test_copy.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seperateHyphenToSentence(s):\n",
    "    parts = s.split('-')\n",
    "    return parts[-1]\n",
    "def getLangText(s):\n",
    "    parts = s.split('-')\n",
    "    return '-'.join(parts[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['langText'] = train_df.sentence_id.apply(getLangText).astype(str)\n",
    "valid_df['langText'] = valid_df.sentence_id.apply(getLangText).astype(str)\n",
    "test_df['langText'] = test_df.sentence_id.apply(getLangText).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['sentence_id'] = pd.factorize(train_df.sentence_id)[0] \n",
    "valid_df['sentence_id'] = pd.factorize(valid_df.sentence_id)[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>word_id</th>\n",
       "      <th>word</th>\n",
       "      <th>FFDAvg</th>\n",
       "      <th>FFDStd</th>\n",
       "      <th>TRTAvg</th>\n",
       "      <th>TRTStd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1544</th>\n",
       "      <td>nl</td>\n",
       "      <td>102</td>\n",
       "      <td>41.0</td>\n",
       "      <td>wasMet</td>\n",
       "      <td>11.353516</td>\n",
       "      <td>0.520182</td>\n",
       "      <td>17.778184</td>\n",
       "      <td>9.606035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1545</th>\n",
       "      <td>nl</td>\n",
       "      <td>103</td>\n",
       "      <td>1.0</td>\n",
       "      <td>U</td>\n",
       "      <td>12.765262</td>\n",
       "      <td>4.057740</td>\n",
       "      <td>21.817248</td>\n",
       "      <td>12.173254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1546</th>\n",
       "      <td>nl</td>\n",
       "      <td>103</td>\n",
       "      <td>2.0</td>\n",
       "      <td>wilt</td>\n",
       "      <td>11.580342</td>\n",
       "      <td>4.493241</td>\n",
       "      <td>15.669324</td>\n",
       "      <td>6.043732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1547</th>\n",
       "      <td>nl</td>\n",
       "      <td>103</td>\n",
       "      <td>3.0</td>\n",
       "      <td>niet</td>\n",
       "      <td>10.348129</td>\n",
       "      <td>3.490696</td>\n",
       "      <td>11.485933</td>\n",
       "      <td>4.261039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1548</th>\n",
       "      <td>nl</td>\n",
       "      <td>103</td>\n",
       "      <td>4.0</td>\n",
       "      <td>spreken?</td>\n",
       "      <td>9.457177</td>\n",
       "      <td>3.943935</td>\n",
       "      <td>10.094739</td>\n",
       "      <td>5.087791</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     language  sentence_id  word_id      word     FFDAvg    FFDStd     TRTAvg  \\\n",
       "1544       nl          102     41.0    wasMet  11.353516  0.520182  17.778184   \n",
       "1545       nl          103      1.0         U  12.765262  4.057740  21.817248   \n",
       "1546       nl          103      2.0      wilt  11.580342  4.493241  15.669324   \n",
       "1547       nl          103      3.0      niet  10.348129  3.490696  11.485933   \n",
       "1548       nl          103      4.0  spreken?   9.457177  3.943935  10.094739   \n",
       "\n",
       "         TRTStd  \n",
       "1544   9.606035  \n",
       "1545  12.173254  \n",
       "1546   6.043732  \n",
       "1547   4.261039  \n",
       "1548   5.087791  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "PRED_DIR = '../../data/task2_predictions/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "MAE for FFDAvg: 6.1910010529538635\n",
      "MAE for FFDStd: 2.3922250632986244\n",
      "MAE for TRTAvg: 9.298877343304254\n",
      "MAE for TRTStd: 5.414986576571236\n",
      "Overall MAE: 5.824272509031994\n",
      "Epoch: 2\n",
      "MAE for FFDAvg: 5.97004458625873\n",
      "MAE for FFDStd: 2.563052424660061\n",
      "MAE for TRTAvg: 8.889530168800226\n",
      "MAE for TRTStd: 5.711820415268701\n",
      "Overall MAE: 5.783611898746929\n",
      "Epoch: 3\n",
      "MAE for FFDAvg: 4.412797070936794\n",
      "MAE for FFDStd: 2.0943542734212275\n",
      "MAE for TRTAvg: 7.707288845538965\n",
      "MAE for TRTStd: 4.916624766203072\n",
      "Overall MAE: 4.7827662390250145\n",
      "Epoch: 4\n",
      "MAE for FFDAvg: 3.865894035578442\n",
      "MAE for FFDStd: 2.009205682674153\n",
      "MAE for TRTAvg: 7.101074165252214\n",
      "MAE for TRTStd: 4.871289151778243\n",
      "Overall MAE: 4.461865758820763\n",
      "Epoch: 5\n",
      "MAE for FFDAvg: 3.3015762514522398\n",
      "MAE for FFDStd: 1.916108703904976\n",
      "MAE for TRTAvg: 6.4523707679402875\n",
      "MAE for TRTStd: 4.675680602929901\n",
      "Overall MAE: 4.086434081556851\n",
      "Epoch: 6\n",
      "MAE for FFDAvg: 2.8503106915565866\n",
      "MAE for FFDStd: 1.904771292658685\n",
      "MAE for TRTAvg: 6.016439683129154\n",
      "MAE for TRTStd: 4.6870428699061275\n",
      "Overall MAE: 3.8646411343126386\n",
      "Epoch: 7\n",
      "MAE for FFDAvg: 2.860131071306565\n",
      "MAE for FFDStd: 1.9318188933020928\n",
      "MAE for TRTAvg: 5.945825947791861\n",
      "MAE for TRTStd: 4.6301939744392895\n",
      "Overall MAE: 3.8419924717099523\n",
      "Epoch: 8\n",
      "MAE for FFDAvg: 2.5757999648057797\n",
      "MAE for FFDStd: 1.8445709720886174\n",
      "MAE for TRTAvg: 5.82592169631043\n",
      "MAE for TRTStd: 4.650980602566452\n",
      "Overall MAE: 3.72431830894282\n",
      "Epoch: 9\n",
      "MAE for FFDAvg: 2.8612046275487284\n",
      "MAE for FFDStd: 1.9152417454101356\n",
      "MAE for TRTAvg: 5.800888405892182\n",
      "MAE for TRTStd: 4.559781565220579\n",
      "Overall MAE: 3.7842790860179063\n",
      "Epoch: 10\n",
      "MAE for FFDAvg: 2.6236603356668686\n",
      "MAE for FFDStd: 1.8715893606875633\n",
      "MAE for TRTAvg: 5.4521405081648044\n",
      "MAE for TRTStd: 4.478455405744168\n",
      "Overall MAE: 3.6064614025658512\n",
      "26\n",
      "torch.Size([16, 162, 4])\n",
      "torch.Size([16, 162])\n",
      "torch.Size([16, 162, 4])\n",
      "torch.Size([16, 162])\n",
      "torch.Size([16, 162, 4])\n",
      "torch.Size([16, 162])\n",
      "torch.Size([16, 162, 4])\n",
      "torch.Size([16, 162])\n",
      "torch.Size([16, 162, 4])\n",
      "torch.Size([16, 162])\n",
      "torch.Size([16, 162, 4])\n",
      "torch.Size([16, 162])\n",
      "torch.Size([16, 162, 4])\n",
      "torch.Size([16, 162])\n",
      "torch.Size([16, 162, 4])\n",
      "torch.Size([16, 162])\n",
      "torch.Size([16, 162, 4])\n",
      "torch.Size([16, 162])\n",
      "torch.Size([16, 162, 4])\n",
      "torch.Size([16, 162])\n",
      "torch.Size([16, 162, 4])\n",
      "torch.Size([16, 162])\n",
      "torch.Size([16, 162, 4])\n",
      "torch.Size([16, 162])\n",
      "torch.Size([16, 162, 4])\n",
      "torch.Size([16, 162])\n",
      "torch.Size([16, 162, 4])\n",
      "torch.Size([16, 162])\n",
      "torch.Size([16, 162, 4])\n",
      "torch.Size([16, 162])\n",
      "torch.Size([16, 162, 4])\n",
      "torch.Size([16, 162])\n",
      "torch.Size([16, 162, 4])\n",
      "torch.Size([16, 162])\n",
      "torch.Size([16, 162, 4])\n",
      "torch.Size([16, 162])\n",
      "torch.Size([16, 162, 4])\n",
      "torch.Size([16, 162])\n",
      "torch.Size([16, 162, 4])\n",
      "torch.Size([16, 162])\n",
      "torch.Size([16, 162, 4])\n",
      "torch.Size([16, 162])\n",
      "torch.Size([16, 162, 4])\n",
      "torch.Size([16, 162])\n",
      "torch.Size([16, 162, 4])\n",
      "torch.Size([16, 162])\n",
      "torch.Size([16, 162, 4])\n",
      "torch.Size([16, 162])\n",
      "torch.Size([16, 162, 4])\n",
      "torch.Size([16, 162])\n",
      "torch.Size([2, 162, 4])\n",
      "torch.Size([2, 162])\n"
     ]
    }
   ],
   "source": [
    "model_trainer = src.model.ModelTrainerNew()\n",
    "model_trainer.train(train_df, valid_df, num_epochs=50,batch_size = 16)\n",
    "predict_df = model_trainer.test(test_df)\n",
    "predict_df.to_csv(PRED_DIR+\"predictions_raw.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PoTeC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "MAE for FFDAvg: 0.5823010735797874\n",
      "MAE for FFDStd: 0.7400588015153581\n",
      "MAE for TRTAvg: 10.526837360074616\n",
      "MAE for TRTStd: 6.175311681908394\n",
      "Overall MAE: 4.506127229269539\n",
      "Epoch: 2\n",
      "MAE for FFDAvg: 0.632071892499246\n",
      "MAE for FFDStd: 0.5072833894676236\n",
      "MAE for TRTAvg: 7.321183295523286\n",
      "MAE for TRTStd: 4.5133615192632535\n",
      "Overall MAE: 3.243475024188352\n",
      "Epoch: 3\n",
      "MAE for FFDAvg: 0.5469121296176844\n",
      "MAE for FFDStd: 0.4815761183732746\n",
      "MAE for TRTAvg: 6.133230318745308\n",
      "MAE for TRTStd: 4.259392664237189\n",
      "Overall MAE: 2.855277807743364\n",
      "Epoch: 4\n",
      "MAE for FFDAvg: 0.5014003681816833\n",
      "MAE for FFDStd: 0.4826375896194039\n",
      "MAE for TRTAvg: 5.910082573044346\n",
      "MAE for TRTStd: 4.088316981782601\n",
      "Overall MAE: 2.745609378157009\n",
      "Epoch: 5\n",
      "MAE for FFDAvg: 0.5087026832107618\n",
      "MAE for FFDStd: 0.4815307669571876\n",
      "MAE for TRTAvg: 5.797355044190964\n",
      "MAE for TRTStd: 4.061848531678238\n",
      "Overall MAE: 2.7123592565092878\n",
      "Epoch: 6\n",
      "MAE for FFDAvg: 0.5746192211749244\n",
      "MAE for FFDStd: 0.5599158476712045\n",
      "MAE for TRTAvg: 5.76013220215616\n",
      "MAE for TRTStd: 4.081425438795796\n",
      "Overall MAE: 2.744023177449521\n",
      "Epoch: 7\n",
      "MAE for FFDAvg: 0.5087214742015512\n",
      "MAE for FFDStd: 0.5545169751473973\n",
      "MAE for TRTAvg: 5.775881671757182\n",
      "MAE for TRTStd: 3.98177411554797\n",
      "Overall MAE: 2.7052235591635254\n",
      "Epoch: 8\n",
      "MAE for FFDAvg: 0.5726374622304296\n",
      "MAE for FFDStd: 0.5760667738331219\n",
      "MAE for TRTAvg: 5.585047001428563\n",
      "MAE for TRTStd: 3.9039027096899175\n",
      "Overall MAE: 2.659413486795508\n",
      "Epoch: 9\n",
      "MAE for FFDAvg: 0.46529925023363566\n",
      "MAE for FFDStd: 0.4694180407954046\n",
      "MAE for TRTAvg: 5.319029916115971\n",
      "MAE for TRTStd: 3.779180029527969\n",
      "Overall MAE: 2.508231809168245\n",
      "Epoch: 10\n",
      "MAE for FFDAvg: 0.5082338422353873\n",
      "MAE for FFDStd: 0.46713019343911605\n",
      "MAE for TRTAvg: 5.472818770846533\n",
      "MAE for TRTStd: 3.804324259661254\n",
      "Overall MAE: 2.5631267665455724\n",
      "26\n",
      "torch.Size([16, 162, 4])\n",
      "torch.Size([16, 162])\n",
      "torch.Size([16, 162, 4])\n",
      "torch.Size([16, 162])\n",
      "torch.Size([16, 162, 4])\n",
      "torch.Size([16, 162])\n",
      "torch.Size([16, 162, 4])\n",
      "torch.Size([16, 162])\n",
      "torch.Size([16, 162, 4])\n",
      "torch.Size([16, 162])\n",
      "torch.Size([16, 162, 4])\n",
      "torch.Size([16, 162])\n",
      "torch.Size([16, 162, 4])\n",
      "torch.Size([16, 162])\n",
      "torch.Size([16, 162, 4])\n",
      "torch.Size([16, 162])\n",
      "torch.Size([16, 162, 4])\n",
      "torch.Size([16, 162])\n",
      "torch.Size([16, 162, 4])\n",
      "torch.Size([16, 162])\n",
      "torch.Size([16, 162, 4])\n",
      "torch.Size([16, 162])\n",
      "torch.Size([16, 162, 4])\n",
      "torch.Size([16, 162])\n",
      "torch.Size([16, 162, 4])\n",
      "torch.Size([16, 162])\n",
      "torch.Size([16, 162, 4])\n",
      "torch.Size([16, 162])\n",
      "torch.Size([16, 162, 4])\n",
      "torch.Size([16, 162])\n",
      "torch.Size([16, 162, 4])\n",
      "torch.Size([16, 162])\n",
      "torch.Size([16, 162, 4])\n",
      "torch.Size([16, 162])\n",
      "torch.Size([16, 162, 4])\n",
      "torch.Size([16, 162])\n",
      "torch.Size([16, 162, 4])\n",
      "torch.Size([16, 162])\n",
      "torch.Size([16, 162, 4])\n",
      "torch.Size([16, 162])\n",
      "torch.Size([16, 162, 4])\n",
      "torch.Size([16, 162])\n",
      "torch.Size([16, 162, 4])\n",
      "torch.Size([16, 162])\n",
      "torch.Size([16, 162, 4])\n",
      "torch.Size([16, 162])\n",
      "torch.Size([16, 162, 4])\n",
      "torch.Size([16, 162])\n",
      "torch.Size([16, 162, 4])\n",
      "torch.Size([16, 162])\n",
      "torch.Size([2, 162, 4])\n",
      "torch.Size([2, 162])\n",
      "GECO-NL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "MAE for FFDAvg: 1.857463881204624\n",
      "MAE for FFDStd: 1.4763231496938045\n",
      "MAE for TRTAvg: 3.69542256427636\n",
      "MAE for TRTStd: 2.8979468367944095\n",
      "Overall MAE: 2.4817891079922996\n",
      "Epoch: 2\n",
      "MAE for FFDAvg: 1.7490215771672422\n",
      "MAE for FFDStd: 1.471343104906308\n",
      "MAE for TRTAvg: 3.3834930639685687\n",
      "MAE for TRTStd: 2.7978862064200034\n",
      "Overall MAE: 2.3504359881155303\n",
      "Epoch: 3\n",
      "MAE for FFDAvg: 1.7382563450885078\n",
      "MAE for FFDStd: 1.4705238698309642\n",
      "MAE for TRTAvg: 3.3679391506895064\n",
      "MAE for TRTStd: 2.7861865163825845\n",
      "Overall MAE: 2.3407264704978905\n",
      "Epoch: 4\n",
      "MAE for FFDAvg: 1.8073718039293414\n",
      "MAE for FFDStd: 1.4941867413130827\n",
      "MAE for TRTAvg: 3.3623832248729206\n",
      "MAE for TRTStd: 2.8424449148450828\n",
      "Overall MAE: 2.376596671240107\n",
      "Epoch: 5\n",
      "MAE for FFDAvg: 1.7604131126502887\n",
      "MAE for FFDStd: 1.4810547185357577\n",
      "MAE for TRTAvg: 3.355635063604232\n",
      "MAE for TRTStd: 2.824189583930291\n",
      "Overall MAE: 2.3553231196801425\n",
      "Epoch: 6\n",
      "MAE for FFDAvg: 1.7486858452350065\n",
      "MAE for FFDStd: 1.470820808309667\n",
      "MAE for TRTAvg: 3.4060222007781764\n",
      "MAE for TRTStd: 2.8317248909963872\n",
      "Overall MAE: 2.3643134363298093\n",
      "Epoch: 7\n",
      "MAE for FFDAvg: 1.7298231612278465\n",
      "MAE for FFDStd: 1.4929852360234688\n",
      "MAE for TRTAvg: 3.354842541501552\n",
      "MAE for TRTStd: 2.7885264678715576\n",
      "Overall MAE: 2.3415443516561063\n",
      "Epoch: 8\n",
      "MAE for FFDAvg: 1.728232513123007\n",
      "MAE for FFDStd: 1.4717696767260278\n",
      "MAE for TRTAvg: 3.345792934880033\n",
      "MAE for TRTStd: 2.8073565760311894\n",
      "Overall MAE: 2.3382879251900643\n",
      "Epoch: 9\n"
     ]
    }
   ],
   "source": [
    "for text_name in text_names[6:]:\n",
    "    print(text_name)\n",
    "    model_trainer = src.model.ModelTrainer(text_name=text_name)\n",
    "    model_trainer.train(train_df, valid_df, num_epochs=10,batch_size = 8)\n",
    "    test_df = test_df[test_df.langText == 'CopCo']\n",
    "    predict_df = model_trainer.test(test_df)\n",
    "    predict_df.to_csv(PRED_DIR+text_name+\"_predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
